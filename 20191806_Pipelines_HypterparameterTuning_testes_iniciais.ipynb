{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comecei a brincar com o hyperopt, comparei com o sklearn random search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definições iniciais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T03:17:01.620175Z",
     "start_time": "2019-07-14T03:16:57.610532Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate, RandomizedSearchCV, train_test_split, StratifiedKFold,cross_val_score\n",
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "from joblib import Memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurando random pra deixar reprodutível"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T03:17:01.640186Z",
     "start_time": "2019-07-14T03:17:01.637183Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "random_global = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T03:17:06.018355Z",
     "start_time": "2019-07-14T03:17:05.679040Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "class VIFFilterTransformer(BaseEstimator, TransformerMixin):\n",
    "\t\n",
    "\tdef __init__(self, max_vif=3.0, max_iter = np.inf, verbose = True):\n",
    "\t\tself.max_vif = max_vif\n",
    "\t\tself.verbose = verbose\n",
    "\t\tself.max_iter = max_iter        \n",
    "\t\t\n",
    "\tdef fit(self, X, y=None):\n",
    "\t\tassert isinstance(X, pd.DataFrame), \"X should be a Pandas Dataframe.\"\n",
    "\t\tremaining_vars = list(X.columns.values)\n",
    "\t\t\n",
    "\t\tself._dropped_vars = []\n",
    "\t\tself._vif_dropped_vars = []\n",
    "\t\t\n",
    "\t\tn_iter = 0\n",
    "\t\t\n",
    "\t\tif self.verbose:\n",
    "\t\t\tprint(f\"\\nLog for VIFFilterTransformer {self.get_params()}\")\n",
    "\t\t\t\n",
    "\t\twhile (len(remaining_vars) > 1) and (n_iter < self.max_iter):\n",
    "\t\t\n",
    "\t\t\tvif_iter = np.diag(np.linalg.pinv(np.corrcoef(X.drop(self._dropped_vars, axis=1).values, rowvar=False))).tolist()\n",
    "\t\t\tmax_vif_iter = max(vif_iter)\n",
    "\n",
    "\t\t\tif max_vif_iter <= self.max_vif:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tdropped_var = remaining_vars[vif_iter.index(max_vif_iter)]\n",
    "\n",
    "\t\t\tif self.verbose:\n",
    "\t\t\t\tprint('Iteration %d, Dropped var: %s, vif = %f\\n' % (n_iter, dropped_var, max_vif_iter))\n",
    "\n",
    "\t\t\tremaining_vars.remove(dropped_var)\n",
    "\t\t\tself._dropped_vars.append(dropped_var)\n",
    "\t\t\tself._vif_dropped_vars.append(max_vif_iter)\n",
    "\n",
    "\t\t\tn_iter += 1\n",
    "\n",
    "\t\tif self.verbose and (n_iter == 0):\n",
    "\t\t\tprint(f\"There are no variables with VIF above {self.max_vif}\")\n",
    "\t\t\n",
    "\t\treturn self\n",
    "    \n",
    "\tdef transform(self, X, y=None):\n",
    "\t\tassert hasattr(self, '_dropped_vars'), \"This vif instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.\"\n",
    "\t\tassert isinstance(X, pd.DataFrame), \"X should be a Pandas Dataframe.\"\n",
    "\t\t\n",
    "\t\treturn X.copy().drop(self._dropped_vars, axis=1)\n",
    "\n",
    "# Extracted from https://github.com/jem1031/pandas-pipelines-custom-transformers/blob/master/code/custom_transformers.py\n",
    "class DFImputer(BaseEstimator, TransformerMixin):\n",
    "\t# Imputer but for pandas DataFrames\n",
    "\n",
    "\tdef __init__(self, strategy='mean'):\n",
    "\t\tself.strategy = strategy\n",
    "\t\tself.imp = None\n",
    "\t\tself.statistics_ = None\n",
    "\n",
    "\tdef fit(self, X, y=None):\n",
    "\t\tself.imp = Imputer(strategy=self.strategy)\n",
    "\t\tself.imp.fit(X)\n",
    "\t\tself.statistics_ = pd.Series(self.imp.statistics_, index=X.columns)\n",
    "\t\treturn self\n",
    "\n",
    "\tdef transform(self, X, y=None):\n",
    "\t\t# assumes X is a DataFrame\n",
    "\t\tXimp = self.imp.transform(X)\n",
    "\t\tXfilled = pd.DataFrame(Ximp, index=X.index, columns=X.columns)\n",
    "\t\treturn Xfilled\n",
    "\t\t\n",
    "# This class don't accept null values\n",
    "class DFVarianceThreshold_rascunho(BaseEstimator, TransformerMixin):\n",
    "\t# VarianceThreshold but for pandas DataFrames\n",
    "\n",
    "\tdef __init__(self, threshold=0.0):\n",
    "\t\tself.threshold = threshold\n",
    "\n",
    "\tdef fit(self, X, y=None):\n",
    "\t\tself.selector = VarianceThreshold(threshold=self.threshold)\n",
    "\t\tself.selector.fit(X)\n",
    "\t\tself.variances_ = pd.Series(self.selector.variances_, index=X.columns)\n",
    "\t\treturn self\n",
    "\n",
    "\tdef transform(self, X, y=None):\n",
    "\t\t# assumes X is a DataFrame\n",
    "\t\treturn pd.DataFrame(self.selector.transform(X), index = X.index, columns=X.columns[self.selector.get_support()].tolist())\n",
    "\t\t# , columns = X.columns)\n",
    "\n",
    "class DFVarianceThreshold(BaseEstimator, TransformerMixin):\n",
    "\t# VarianceThreshold but for pandas DataFrames\n",
    "\n",
    "\tdef __init__(self, threshold=0.0, verbose=True):\n",
    "\t\tself.threshold = threshold\n",
    "\t\tself.verbose = verbose\n",
    "\n",
    "\tdef fit(self, X, y=None):\n",
    "\t\tself.variances_ = X.var()\n",
    "\t\tself.selected_cols_ = self.variances_[self.variances_ > self.threshold].index.tolist()\n",
    "\t\t\n",
    "\t\tif self.verbose:\n",
    "\t\t\tprint(f\"\\nLog for DFVarianceThreshold {self.get_params()}\")\n",
    "\t\t\tdropped_cols = set(X.columns) - set(self.selected_cols_)\n",
    "\t\t\tif len(dropped_cols) == 0:\n",
    "\t\t\t\tprint(f\"There are no vars with variance lower than the threshold of {self.threshold}\")\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(f\"The following variables with variance less or equal the {self.threshold} threshold were removed:\")\n",
    "\t\t\t\tprint(self.variances_[self.variances_ <= self.threshold])\n",
    "\t\t\n",
    "\t\treturn self\n",
    "\n",
    "\tdef transform(self, X, y=None):\n",
    "\t\t# assumes X is a DataFrame\n",
    "\t\treturn pd.DataFrame(X[self.selected_cols_], index = X.index, columns = self.selected_cols_)\n",
    "\t\t# return pd.DataFrame(self.selector.transform(X), index = X.index, columns=X.columns[self.selector.get_support()].tolist())\n",
    "\t\t# , columns = X.columns)\n",
    "\t\t\n",
    "class PipelineCheckpoint(BaseEstimator, TransformerMixin):\t\n",
    "\t'''Chekcpoint, only gives type and dimensions of object on given point'''\n",
    "\tdef __init__(self, print_on_fit=True, print_on_transform=True):\n",
    "\t\tself.print_on_fit = print_on_fit\n",
    "\t\tself.print_on_transform = print_on_transform\n",
    "\t\tpass\n",
    "\t\n",
    "\tdef fit(self, X, y=None):\n",
    "\t\tif self.print_on_fit:\n",
    "\t\t\tprint(f\"\\nLog for PipelineCheckpoint.fit\")\n",
    "\t\t\tprint(X.info())\n",
    "\t\treturn self\n",
    "\t\n",
    "\tdef transform(self, X, y=None):\n",
    "\t\tif self.print_on_transform:\n",
    "\t\t\tprint(f\"\\nLog for PipelineCheckpoint.transform\")\n",
    "\t\t\tprint(X.info())\n",
    "\t\treturn pd.DataFrame(X, index=X.index, columns=X.columns)\n",
    "\n",
    "class DFCorrelationFilter(BaseEstimator, TransformerMixin):\n",
    "\t'''Removes pairwise correlated features. By default removes perfect correlated features, e.g. with correlation with absolute value equal to 1.0\n",
    "\t\t\n",
    "\t\tIdea extracted from https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/\n",
    "\t'''\n",
    "\n",
    "\tdef __init__(self, threshold=1.0, verbose=True):\n",
    "\t\tself.threshold = threshold\n",
    "\t\tself.verbose = verbose\n",
    "\t\t\n",
    "\tdef fit(self, X, y=None):\n",
    "\t\t# Create correlation matrix\n",
    "\t\tcorr_matrix = X.corr().abs()\n",
    "\n",
    "\t\t# Select upper triangle of correlation matrix\n",
    "\t\tupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "\t\t# Find index of feature columns with correlation greater than 0.95\n",
    "\t\tself.to_drop_ = [column for column in upper.columns if any(upper[column] >= self.threshold)]\n",
    "\t\t\n",
    "\t\tif self.verbose:\n",
    "\t\t\tprint(f\"\\nLog for DFCorrelationFilter {self.get_params()}\")\n",
    "\t\t\tprint(f\"The following variables have correlation greater or equal than then {self.threshold} threshold and will be dropped\")\n",
    "\t\t\tprint(self.to_drop_)\n",
    "\t\t\n",
    "\t\treturn self\n",
    "\t\n",
    "\tdef transform(self, X, y=None):\n",
    "\t\treturn X.copy().drop(self.to_drop_, axis=1)\n",
    "\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carrega dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T03:18:24.067064Z",
     "start_time": "2019-07-14T03:18:24.063062Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 3, ..., 7.25, nan, 'S'],\n",
       "       [2, 1, 1, ..., 71.2833, 'C85', 'C'],\n",
       "       [3, 1, 3, ..., 7.925, nan, 'S'],\n",
       "       ...,\n",
       "       [889, 0, 3, ..., 23.45, nan, 'S'],\n",
       "       [890, 1, 1, ..., 30.0, 'C148', 'C'],\n",
       "       [891, 0, 3, ..., 7.75, nan, 'Q']], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_val.values.astype('O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T03:19:56.624954Z",
     "start_time": "2019-07-14T03:19:56.582917Z"
    }
   },
   "outputs": [],
   "source": [
    "from category_encoders.hashing import HashingEncoder\n",
    "from category_encoders.sum_coding import SumEncoder\n",
    "from category_encoders.woe import WOEEncoder\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "from category_encoders.one_hot import OneHotEncoder\n",
    "from category_encoders.binary import BinaryEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T03:22:09.051119Z",
     "start_time": "2019-07-14T03:22:09.023094Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived_1</th>\n",
       "      <th>survived_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived_1  survived_2\n",
       "0             1           0\n",
       "1             0           1\n",
       "2             0           1\n",
       "3             0           1\n",
       "4             1           0\n",
       "5             1           0\n",
       "6             1           0\n",
       "7             1           0\n",
       "8             0           1\n",
       "9             0           1\n",
       "10            0           1\n",
       "11            0           1\n",
       "12            1           0\n",
       "13            1           0\n",
       "14            1           0\n",
       "15            0           1\n",
       "16            1           0\n",
       "17            0           1\n",
       "18            1           0\n",
       "19            0           1\n",
       "20            1           0\n",
       "21            0           1\n",
       "22            0           1\n",
       "23            0           1\n",
       "24            1           0\n",
       "25            0           1\n",
       "26            1           0\n",
       "27            1           0\n",
       "28            0           1\n",
       "29            1           0\n",
       "..          ...         ...\n",
       "861           1           0\n",
       "862           0           1\n",
       "863           1           0\n",
       "864           1           0\n",
       "865           0           1\n",
       "866           0           1\n",
       "867           1           0\n",
       "868           1           0\n",
       "869           0           1\n",
       "870           1           0\n",
       "871           0           1\n",
       "872           1           0\n",
       "873           1           0\n",
       "874           0           1\n",
       "875           0           1\n",
       "876           1           0\n",
       "877           1           0\n",
       "878           1           0\n",
       "879           0           1\n",
       "880           0           1\n",
       "881           1           0\n",
       "882           1           0\n",
       "883           1           0\n",
       "884           1           0\n",
       "885           1           0\n",
       "886           1           0\n",
       "887           0           1\n",
       "888           1           0\n",
       "889           0           1\n",
       "890           1           0\n",
       "\n",
       "[891 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OneHotEncoder().fit_transform(df_train_val[['survived']].astype('O'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T03:21:17.325034Z",
     "start_time": "2019-07-14T03:21:17.311022Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embarked</th>\n",
       "      <th>survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Q</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Q</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Q</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Q</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>Q</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>Q</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    embarked  survived\n",
       "0          S         0\n",
       "1          C         1\n",
       "2          S         1\n",
       "3          S         1\n",
       "4          S         0\n",
       "5          Q         0\n",
       "6          S         0\n",
       "7          S         0\n",
       "8          S         1\n",
       "9          C         1\n",
       "10         S         1\n",
       "11         S         1\n",
       "12         S         0\n",
       "13         S         0\n",
       "14         S         0\n",
       "15         S         1\n",
       "16         Q         0\n",
       "17         S         1\n",
       "18         S         0\n",
       "19         C         1\n",
       "20         S         0\n",
       "21         S         1\n",
       "22         Q         1\n",
       "23         S         1\n",
       "24         S         0\n",
       "25         S         1\n",
       "26         C         0\n",
       "27         S         0\n",
       "28         Q         1\n",
       "29         S         0\n",
       "..       ...       ...\n",
       "861        S         0\n",
       "862        S         1\n",
       "863        S         0\n",
       "864        S         0\n",
       "865        S         1\n",
       "866        C         1\n",
       "867        S         0\n",
       "868        S         0\n",
       "869        S         1\n",
       "870        S         0\n",
       "871        S         1\n",
       "872        S         0\n",
       "873        S         0\n",
       "874        C         1\n",
       "875        C         1\n",
       "876        S         0\n",
       "877        S         0\n",
       "878        S         0\n",
       "879        C         1\n",
       "880        S         1\n",
       "881        S         0\n",
       "882        S         0\n",
       "883        S         0\n",
       "884        S         0\n",
       "885        Q         0\n",
       "886        S         0\n",
       "887        S         1\n",
       "888        S         0\n",
       "889        C         1\n",
       "890        Q         0\n",
       "\n",
       "[891 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_val[['embarked', 'survived']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T03:17:11.855391Z",
     "start_time": "2019-07-14T03:17:11.831369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print shape df_train_val: (891, 12)\n",
      "Print shape df_test: (418, 11)\n"
     ]
    }
   ],
   "source": [
    "df_train_val = pd.read_csv(\"./data/train.csv\")\n",
    "df_test = pd.read_csv(\"./data/test.csv\")\n",
    "df_train_val.columns = [x.lower() for x in df_train_val.columns]\n",
    "df_test.columns = [x.lower() for x in df_test.columns]\n",
    "\n",
    "print(f'Print shape df_train_val: {df_train_val.shape}')\n",
    "print(f'Print shape df_test: {df_test.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criação de algumas features novas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T21:11:06.860905Z",
     "start_time": "2019-07-09T21:11:06.614097Z"
    }
   },
   "outputs": [],
   "source": [
    "class ColumnExtractor(TransformerMixin):\n",
    "\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # stateless transformer\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # assumes X is a DataFrame\n",
    "        Xcols = X[self.cols]\n",
    "        return Xcols\n",
    "    \n",
    "class CreateFamilySize(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "#         return X['sibsp'].values + X['parch'].values\n",
    "#         return X.iloc[:,0].values + X.iloc[:,1].values\n",
    "#         return X[['sibsp']] + X[['parch']]\n",
    "#         return (X['sibsp'] + X['parch']).ravel()\n",
    "#         return np.ndarray(shape=(X['sibsp'].values + X['parch'].values)\n",
    "\n",
    "#         family_size = X['sibsp'] +  X['parch']\n",
    "        family_size = (X[['parch', 'sibsp']]).sum(axis=1, skipna=True).fillna(0)\n",
    "        return pd.DataFrame(family_size)\n",
    "    \n",
    "# CreateFamilySize().fit_transform(df_train[['parch', 'sibsp']])\n",
    "\n",
    "# Isso aqui faria sentido se fosse uma variável que fosse criada usando dados do treino, mas aqui não faz sentido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tamanho de família"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T21:11:07.056734Z",
     "start_time": "2019-07-09T21:11:06.861915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family_size</th>\n",
       "      <th>size</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>537</td>\n",
       "      <td>0.303538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>161</td>\n",
       "      <td>0.552795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>102</td>\n",
       "      <td>0.578431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>0.724138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>0.136364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   family_size  size      mean\n",
       "0            1   537  0.303538\n",
       "1            2   161  0.552795\n",
       "2            3   102  0.578431\n",
       "3            4    29  0.724138\n",
       "4            5    15  0.200000\n",
       "5            6    22  0.136364\n",
       "6            7    12  0.333333\n",
       "7            8     6  0.000000\n",
       "8           11     7  0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "family_size\n",
       "1     253\n",
       "2      74\n",
       "3      57\n",
       "4      14\n",
       "5       7\n",
       "6       3\n",
       "7       4\n",
       "8       2\n",
       "11      4\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_data = [df_train_val, df_test]\n",
    "for dataset in full_data:\n",
    "    dataset['family_size'] = 1 + dataset['parch'] + dataset['sibsp']\n",
    "    if 'survived' in dataset.columns:\n",
    "        display(dataset.fillna(-999).groupby('family_size')['survived'].agg(['size', 'mean']).reset_index())\n",
    "    else:\n",
    "        display(dataset.fillna(-999).groupby('family_size').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aparentemente depois de 4 familiares, talvez seja bom juntar depois"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrai o título"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T21:11:07.216881Z",
     "start_time": "2019-07-09T21:11:07.057735Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>size</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sir</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Countess</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ms</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Mme</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lady</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mlle</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Mrs</td>\n",
       "      <td>125</td>\n",
       "      <td>0.792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Miss</td>\n",
       "      <td>182</td>\n",
       "      <td>0.697802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Master</td>\n",
       "      <td>40</td>\n",
       "      <td>0.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Col</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Major</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dr</td>\n",
       "      <td>7</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mr</td>\n",
       "      <td>517</td>\n",
       "      <td>0.156673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jonkheer</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Don</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Rev</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Capt</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       title  size      mean\n",
       "16       Sir     1  1.000000\n",
       "2   Countess     1  1.000000\n",
       "14        Ms     1  1.000000\n",
       "11       Mme     1  1.000000\n",
       "6       Lady     1  1.000000\n",
       "10      Mlle     2  1.000000\n",
       "13       Mrs   125  0.792000\n",
       "9       Miss   182  0.697802\n",
       "8     Master    40  0.575000\n",
       "1        Col     2  0.500000\n",
       "7      Major     2  0.500000\n",
       "4         Dr     7  0.428571\n",
       "12        Mr   517  0.156673\n",
       "5   Jonkheer     1  0.000000\n",
       "3        Don     1  0.000000\n",
       "15       Rev     6  0.000000\n",
       "0       Capt     1  0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "title\n",
       "Col         2\n",
       "Dona        1\n",
       "Dr          1\n",
       "Master     21\n",
       "Miss       78\n",
       "Mr        240\n",
       "Mrs        72\n",
       "Ms          1\n",
       "Rev         2\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_data = [df_train_val, df_test]\n",
    "for dataset in full_data:\n",
    "    dataset['title'] = dataset['name'].str.findall('([A-Z][a-z]+)\\.').map(lambda x: x[0])\n",
    "    if 'survived' in dataset.columns:\n",
    "        display(dataset.fillna(-999).groupby('title')['survived'].agg(['size', 'mean']).reset_index().sort_values(by='mean', ascending=False))\n",
    "    else:\n",
    "        display(dataset.fillna(-999).groupby('title').size())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T02:19:52.456496Z",
     "start_time": "2019-07-09T02:19:52.417461Z"
    }
   },
   "source": [
    "## Acha aspas ou parênteses no nome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T21:11:07.392048Z",
     "start_time": "2019-07-09T21:11:07.219882Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_aspas</th>\n",
       "      <th>size</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>0.716981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>838</td>\n",
       "      <td>0.362768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name_aspas  size      mean\n",
       "1           1    53  0.716981\n",
       "0           0   838  0.362768"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_parenteses</th>\n",
       "      <th>size</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>143</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>748</td>\n",
       "      <td>0.310160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name_parenteses  size      mean\n",
       "1                1   143  0.769231\n",
       "0                0   748  0.310160"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "name_aspas\n",
       "0    396\n",
       "1     22\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "name_parenteses\n",
       "0    340\n",
       "1     78\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_data = [df_train_val, df_test]\n",
    "for dataset in full_data:\n",
    "    dataset['name_aspas'] = dataset['name'].str.findall('\\\"').map(lambda x: len(x) >= 1 ).astype('int')\n",
    "    dataset['name_parenteses'] = dataset['name'].str.findall('\\(').map(lambda x: len(x) >= 1 ).astype('int')\n",
    "    if 'survived' in dataset.columns:\n",
    "        display(dataset.fillna(-999).groupby('name_aspas')['survived'].agg(['size', 'mean']).reset_index().sort_values(by='mean', ascending=False))\n",
    "        display(dataset.fillna(-999).groupby('name_parenteses')['survived'].agg(['size', 'mean']).reset_index().sort_values(by='mean', ascending=False))\n",
    "    else:\n",
    "        display(dataset.fillna(-999).groupby('name_aspas').size())\n",
    "        display(dataset.fillna(-999).groupby('name_parenteses').size())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separa as amostras para treino e validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T21:11:18.653790Z",
     "start_time": "2019-07-09T21:11:18.645784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print shape df_train: (791, 16)\n",
      "Print shape df_val: (100, 16)\n"
     ]
    }
   ],
   "source": [
    "df_train, df_val = train_test_split(df_train_val, test_size = 100, shuffle=True, stratify=df_train_val['survived'], random_state=random_global)\n",
    "print(f'Print shape df_train: {df_train.shape}')\n",
    "print(f'Print shape df_val: {df_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T03:38:06.308453Z",
     "start_time": "2019-06-28T03:38:06.304449Z"
    }
   },
   "source": [
    "# Definição do pipeline básico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T21:11:11.656158Z",
     "start_time": "2019-07-09T21:11:11.653165Z"
    }
   },
   "outputs": [],
   "source": [
    "num_cols = ['age', 'sibsp', 'parch', 'fare', 'family_size']\n",
    "cat_cols = ['sex', 'embarked', 'title', 'name_aspas', 'name_parenteses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T21:11:12.386839Z",
     "start_time": "2019-07-09T21:11:12.381844Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_pipeline = make_pipeline(SimpleImputer(strategy='constant', fill_value='CAT_MISSING'), OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "num_pipeline = make_pipeline(SimpleImputer(strategy='median'))\n",
    "full_pipe = Pipeline([('preprocessing', \n",
    "                        make_column_transformer(\n",
    "                            (cat_pipeline, cat_cols)\n",
    "                            ,(num_pipeline, num_cols)))\n",
    "                    ,('clf', RandomForestClassifier(random_state=random_global, n_estimators=100))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T21:11:13.368708Z",
     "start_time": "2019-07-09T21:11:13.364714Z"
    }
   },
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T21:11:21.184630Z",
     "start_time": "2019-07-09T21:11:20.381909Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score médio nas partições de treino : 0.9982494643370107\n",
      "score médio nas partições de validação : 0.8527953182286943\n",
      "score no teste: 0.8367996604414261\n"
     ]
    }
   ],
   "source": [
    "crossval = cross_validate(full_pipe, df_train, df_train['survived'], return_train_score=True, cv=kf, scoring='roc_auc')\n",
    "\n",
    "print(f\"score médio nas partições de treino : {crossval['train_score'].mean()}\")\n",
    "print(f\"score médio nas partições de validação : {crossval['test_score'].mean()}\")\n",
    "      \n",
    "# No teste\n",
    "full_pipe.fit(df_train, df_train['survived'])\n",
    "print(f\"score no teste: {roc_auc_score(df_val['survived'], full_pipe.predict_proba(df_val)[:,1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-04T05:15:54.991054Z",
     "start_time": "2019-07-04T05:15:54.183621Z"
    }
   },
   "source": [
    "Esse é o score antigo sem criar nenhuma feature\n",
    "\n",
    "score médio nas partições de treino : 0.9979837284291866\n",
    "\n",
    "score médio nas partições de validação : 0.8600773224618562\n",
    "\n",
    "score no teste: 0.8247028862478777"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como usar opção memory no Pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T05:50:20.690280Z",
     "start_time": "2019-06-30T05:50:20.686275Z"
    }
   },
   "outputs": [],
   "source": [
    "# Teste do memory\n",
    "class sleep_on_fit(BaseEstimator, TransformerMixin):\n",
    "\t# Imputer but for pandas DataFrames\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tpass        \n",
    "\n",
    "\tdef fit(self, X, y=None):\n",
    "\t\ttime.sleep(2)\n",
    "\t\treturn self\n",
    "\n",
    "\tdef transform(self, X, y=None):\n",
    "\t\t# assumes X is a DataFrame\n",
    "\t\treturn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T05:50:20.814392Z",
     "start_time": "2019-06-30T05:50:20.691280Z"
    }
   },
   "outputs": [],
   "source": [
    "cachedir = mkdtemp(dir='./')\n",
    "memory = Memory(location=cachedir, verbose=10)\n",
    "pipe_teste = make_pipeline(sleep_on_fit(), memory=memory)\n",
    "rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-29T20:50:44.309174Z",
     "start_time": "2019-06-29T20:50:44.306181Z"
    }
   },
   "source": [
    "# Testando diferentes estratégias para tunar os hiperparâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T03:40:48.000774Z",
     "start_time": "2019-06-28T03:40:47.998773Z"
    }
   },
   "source": [
    "## Random grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T02:40:59.782510Z",
     "start_time": "2019-07-09T02:40:59.775513Z"
    }
   },
   "outputs": [],
   "source": [
    "random_gs_param_grid = {'clf__bootstrap': [False, True]\n",
    ",'clf__class_weight': ['balanced', 'balanced_subsample', None]\n",
    ",'clf__criterion': ['gini', 'entropy']\n",
    ",'clf__max_depth': list(range(1,11))\n",
    ",'clf__max_features': ['auto','sqrt', None]\n",
    ",'clf__max_leaf_nodes': [x for x in range(2,2**10 +1)]\n",
    "# ,'clf__min_impurity_decrease': [None]\n",
    "# ,'clf__min_impurity_split': [None]\n",
    "# ,'clf__min_samples_leaf': [None]\n",
    ",'clf__min_samples_split': [2 ** x for x in range(1,11)]\n",
    ",'clf__min_weight_fraction_leaf': [x * 0.1 for x in range(1,6)]\n",
    ",'clf__n_estimators': [x * 100 for x in range(1,11)]\n",
    ",'clf__n_jobs': [-1]\n",
    "# ,'clf__oob_score': [False]\n",
    "# ,'clf__random_state': [None]\n",
    "# ,'clf__warm_start': [False]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T02:45:06.946581Z",
     "start_time": "2019-07-09T02:41:01.559674Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\T-Gamer\\Anaconda3\\envs\\sklearn_novo\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=StratifiedKFold(n_splits=5, random_state=42, shuffle=True),\n",
       "                   error_score='raise-deprecating',\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('preprocessing',\n",
       "                                              ColumnTransformer(n_jobs=None,\n",
       "                                                                remainder='drop',\n",
       "                                                                sparse_threshold=0.3,\n",
       "                                                                transformer_weights=None,\n",
       "                                                                transformers=[('pipeline-1',\n",
       "                                                                               Pipeline(memory=None,\n",
       "                                                                                        steps=[('simpleimputer',\n",
       "                                                                                                SimpleImputer(add_...\n",
       "                                                                20, 21, 22, 23,\n",
       "                                                                24, 25, 26, 27,\n",
       "                                                                28, 29, 30, 31, ...],\n",
       "                                        'clf__min_samples_split': [2, 4, 8, 16,\n",
       "                                                                   32, 64, 128,\n",
       "                                                                   256, 512,\n",
       "                                                                   1024],\n",
       "                                        'clf__min_weight_fraction_leaf': [0.1,\n",
       "                                                                          0.2,\n",
       "                                                                          0.30000000000000004,\n",
       "                                                                          0.4,\n",
       "                                                                          0.5],\n",
       "                                        'clf__n_estimators': [100, 200, 300,\n",
       "                                                              400, 500, 600,\n",
       "                                                              700, 800, 900,\n",
       "                                                              1000],\n",
       "                                        'clf__n_jobs': [-1]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=1, refit=True,\n",
       "                   return_train_score=False, scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_gs = RandomizedSearchCV(param_distributions=random_gs_param_grid, estimator=full_pipe, n_iter=100, cv = kf, random_state=1, scoring='roc_auc')\n",
    "random_gs.fit(df_train, df_train['survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T03:18:57.441148Z",
     "start_time": "2019-07-09T03:18:57.419129Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('preprocessing',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('pipeline-1',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value='CAT_MISSING',\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='constant',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('onehotencoder',\n",
       "                                                                   On...\n",
       "                 RandomForestClassifier(bootstrap=True,\n",
       "                                        class_weight='balanced_subsample',\n",
       "                                        criterion='entropy', max_depth=2,\n",
       "                                        max_features=None, max_leaf_nodes=637,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=8,\n",
       "                                        min_weight_fraction_leaf=0.1,\n",
       "                                        n_estimators=900, n_jobs=-1,\n",
       "                                        oob_score=False, random_state=42,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coloca melhores parâmetros no pipe\n",
    "full_pipe.set_params(**random_gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T03:19:07.147789Z",
     "start_time": "2019-07-09T03:18:58.266667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score médio nas partições de treino : 0.85921131797813\n",
      "score médio nas partições de validação : 0.8461187354779668\n",
      "AUC treino: 0.854030\n",
      "AUC validação: 0.806452\n"
     ]
    }
   ],
   "source": [
    "# Cross validation no treino\n",
    "crossval = cross_validate(full_pipe, df_train, df_train['survived'], return_train_score=True, cv=kf, scoring='roc_auc')\n",
    "\n",
    "print(f\"score médio nas partições de treino : {crossval['train_score'].mean()}\")\n",
    "print(f\"score médio nas partições de validação : {crossval['test_score'].mean()}\")\n",
    "      \n",
    "full_pipe.fit(df_train, df_train['survived'])\n",
    "\n",
    "print('AUC treino: %f' % roc_auc_score(df_train['survived'], full_pipe.predict_proba(df_train)[:,1]))\n",
    "print('AUC validação: %f' % roc_auc_score(df_val['survived'], full_pipe.predict_proba(df_val)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Grid search na mão pra usar tqdm progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T18:58:58.455588Z",
     "start_time": "2019-06-30T18:58:58.443577Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_gs_param_grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-302-6488c506c078>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mbest_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearch_tqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_gs_param_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'random_gs_param_grid' is not defined"
     ]
    }
   ],
   "source": [
    "def GridSearch_tqdm(estimator, params_grid, n_iter):\n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "    for param in tqdm(ParameterSampler(params_grid, n_iter=n_iter)):\n",
    "        \n",
    "#         print(param)\n",
    "    \n",
    "        crossval = cross_validate(full_pipe, df_train, df_train['survived'], return_train_score=True, cv=kf, scoring='roc_auc')\n",
    "        \n",
    "        score_iter = crossval['test_score'].mean()\n",
    "        if score_iter > best_score:\n",
    "            best_score = score_iter\n",
    "            best_params = param\n",
    "            print(best_score, best_params)\n",
    "#         print(crossval['test_score'].mean())\n",
    "    \n",
    "    print(best_score, best_params)\n",
    "    \n",
    "    return {'best_params':best_params, 'best_score':best_score}\n",
    "    \n",
    "    \n",
    "best_params = GridSearch_tqdm(estimator=None, params_grid=random_gs_param_grid, n_iter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-29T20:27:02.195822Z",
     "start_time": "2019-06-29T20:27:02.192818Z"
    }
   },
   "source": [
    "## Random grid search com oob score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-29T20:53:13.454971Z",
     "start_time": "2019-06-29T20:53:13.451968Z"
    }
   },
   "source": [
    "Pra esse aqui não vai ter cross validation, vou usar o oob escore da Random Forest. Necessariamente vamos precisar de bootstrap na random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse aqui o preprocessing foi feito em tudo, ao contrário dos outros com cross-validation, em que o pipeline garante que as transformações são fitadas nas partições de treino e aplicadas nas de validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T02:53:47.487935Z",
     "start_time": "2019-07-09T02:53:45.650285Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_params': {'oob_score': True,\n",
       "  'n_jobs': -1,\n",
       "  'n_estimators': 700,\n",
       "  'min_weight_fraction_leaf': 0.4,\n",
       "  'min_samples_split': 2,\n",
       "  'max_leaf_nodes': 982,\n",
       "  'max_features': None,\n",
       "  'max_depth': 7,\n",
       "  'criterion': 'entropy',\n",
       "  'class_weight': 'balanced_subsample',\n",
       "  'bootstrap': True},\n",
       " 'best_score': 0.7850821744627055}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_forest_oob = {'bootstrap': [True]\n",
    ",'class_weight': ['balanced', 'balanced_subsample', None]\n",
    ",'criterion': ['gini', 'entropy']\n",
    ",'max_depth': list(range(1,11))\n",
    ",'max_features': ['auto','sqrt', None]\n",
    ",'max_leaf_nodes': [x for x in range(2,2**10 +1)]\n",
    "# ,'min_impurity_decrease': [None]\n",
    "# ,'min_impurity_split': [None]\n",
    "# ,'min_samples_leaf': [None]\n",
    ",'min_samples_split': [2 ** x for x in range(1,11)]\n",
    ",'min_weight_fraction_leaf': [x * 0.1 for x in range(1,6)]\n",
    ",'n_estimators': [x * 100 for x in range(1,11)]\n",
    ",'n_jobs': [-1]\n",
    ",'oob_score': [True]\n",
    "# ,'random_state': [None]\n",
    "# ,'warm_start': [False]\n",
    "}\n",
    "\n",
    "def GridSearch_oob(params_grid, n_iter):\n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "    X = Pipeline([('preprocessing', make_column_transformer((cat_pipeline, cat_cols),(num_pipeline, num_cols)))]).fit_transform(df_train)\n",
    "    \n",
    "    i=0\n",
    "    for param in tqdm(ParameterSampler(params_grid, n_iter=n_iter, random_state=random_global)):\n",
    "        i+=1\n",
    "        clf = RandomForestClassifier(**param)\n",
    "        clf.oob_score  = True\n",
    "        clf.bootstrap  = True\n",
    "        \n",
    "        clf.fit(X, df_train['survived'])\n",
    "        \n",
    "        score_iter = clf.oob_score_\n",
    "\n",
    "        if score_iter > best_score:\n",
    "            best_score = score_iter\n",
    "            best_params = param\n",
    "#             print(\"Iteração %d\" % i)\n",
    "#             print(best_score, best_params)\n",
    "#         print(crossval['test_score'].mean())\n",
    "    \n",
    "#     print(best_score, best_params)\n",
    "    \n",
    "    return {'best_params':best_params, 'best_score':best_score}\n",
    "GridSearch_oob(grid_forest_oob, n_iter = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T03:11:23.322382Z",
     "start_time": "2019-07-09T03:00:54.146187Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [10:29<00:00,  2.05it/s]\n"
     ]
    }
   ],
   "source": [
    "best_params_oob = GridSearch_oob(grid_forest_oob, n_iter = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T03:19:32.644917Z",
     "start_time": "2019-07-09T03:19:32.639912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced_subsample',\n",
       "                       criterion='entropy', max_depth=9, max_features='sqrt',\n",
       "                       max_leaf_nodes=42, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=16, min_weight_fraction_leaf=0.1,\n",
       "                       n_estimators=900, n_jobs=-1, oob_score=True,\n",
       "                       random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_pipe.named_steps['clf'].set_params(**best_params_oob['best_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T03:19:40.213741Z",
     "start_time": "2019-07-09T03:19:33.627752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score médio nas partições de treino : 0.8551928346406388\n",
      "score médio nas partições de validação : 0.8436748087776602\n",
      "AUC treino: 0.853328\n",
      "AUC validação: 0.826825\n"
     ]
    }
   ],
   "source": [
    "# Cross validation no treino\n",
    "crossval = cross_validate(full_pipe, df_train, df_train['survived'], return_train_score=True, cv=kf, scoring='roc_auc')\n",
    "\n",
    "print(f\"score médio nas partições de treino : {crossval['train_score'].mean()}\")\n",
    "print(f\"score médio nas partições de validação : {crossval['test_score'].mean()}\")\n",
    "      \n",
    "full_pipe.fit(df_train, df_train['survived'])\n",
    "\n",
    "print('AUC treino: %f' % roc_auc_score(df_train['survived'], full_pipe.predict_proba(df_train)[:,1]))\n",
    "print('AUC validação: %f' % roc_auc_score(df_val['survived'], full_pipe.predict_proba(df_val)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T04:52:58.188014Z",
     "start_time": "2019-06-30T04:52:58.185012Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: remover combinações ilegais para os parâmetros. Ver o que vem primeiro, max_leaf_nodes or max_depth\n",
    "# Não consegui fazer de um jeito eficiente e sem travar. Vou fazer isso depois"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-29T20:35:19.025385Z",
     "start_time": "2019-06-29T20:35:19.017378Z"
    }
   },
   "source": [
    "0.7914032869785083 {'n_jobs': -1, 'n_estimators': 800, 'min_weight_fraction_leaf': 0.2, 'min_samples_split': 256, 'max_leaf_nodes': 423, 'max_features': None, 'max_depth': 3, 'criterion': 'gini', 'class_weight': None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando com Hyperopt TPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T21:11:32.806405Z",
     "start_time": "2019-07-09T21:11:32.549501Z"
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, hp, tpe, rand, Trials, space_eval, STATUS_OK, anneal\n",
    "from hyperopt.pyll import scope as ho_scope\n",
    "from hyperopt.pyll.stochastic import sample as ho_sample\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T21:11:36.798783Z",
     "start_time": "2019-07-09T21:11:36.789775Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter space\n",
    "params_grid_ho = {\n",
    "    'clf__bootstrap': hp.choice('clf__bootstrap', [False, True]),\n",
    "    'clf__class_weight': hp.choice('clf__class_weight',['balanced', 'balanced_subsample', None]),\n",
    "    'clf__criterion': hp.choice('clf__criterion', ['gini', 'entropy']),\n",
    "    'clf__max_depth': ho_scope.int(hp.uniform('clf__max_depth', low=1, high=11)),\n",
    "    'clf__max_features': hp.choice('clf__max_features', ['auto','sqrt', None]),\n",
    "    'clf__max_leaf_nodes': ho_scope.int(hp.uniform('clf__max_leaf_nodes', low=2, high=1024)),\n",
    "    'clf__min_samples_split': ho_scope.int(hp.uniform('clf__min_samples_split', low=2, high=1024)),\n",
    "    'clf__min_weight_fraction_leaf': 0.1 * ho_scope.int(hp.uniform('clf__min_weight_fraction_leaf', low=0, high=5)),\n",
    "    'clf__n_estimators': 100 * ho_scope.int(hp.uniform('clf__n_estimators', low=1, high=10)),\n",
    "    'clf__oob_score': False\n",
    "    }\n",
    "\n",
    "# Função a ser minimizada\n",
    "def f_to_min1(hps, model, X, y, cv):\n",
    "    \"\"\"\n",
    "    Target function for optimization\n",
    "    \n",
    "    Parameters:\n",
    "    ----------------\n",
    "    hps : sample point from search space\n",
    "    X : feature matrix\n",
    "    y : target array\n",
    "    ncv : number of folds for cross-validation\n",
    "    \n",
    "    Returns:\n",
    "    ----------------\n",
    "    : target function value (negative mean cross-val ROC-AUC score)\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    model.set_params(**hps)\n",
    "        \n",
    "    crossval = cross_validate(model, X, y, return_train_score=True, cv=cv, scoring='roc_auc')\n",
    "\n",
    "    return {'loss': -crossval['test_score'].mean(), 'status': STATUS_OK}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T21:40:51.657302Z",
     "start_time": "2019-07-09T21:16:16.053822Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 500/500 [24:35<00:00,  2.16s/it, best loss: -0.872310551058348]\n"
     ]
    }
   ],
   "source": [
    "trials_clf1 = Trials()\n",
    "\n",
    "\n",
    "best_clf1=fmin(partial(f_to_min1,model=full_pipe, X=df_train, y=df_train['survived'], cv=kf)\n",
    "     ,params_grid_ho, algo=tpe.suggest, max_evals=500, trials=trials_clf1, rstate=np.random.RandomState(random_global))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_estimators pra pegar todos modelos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T00:41:16.944418Z",
     "start_time": "2019-07-10T00:41:16.939414Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__bootstrap': True,\n",
       " 'clf__class_weight': 'balanced',\n",
       " 'clf__criterion': 'entropy',\n",
       " 'clf__max_depth': 7,\n",
       " 'clf__max_features': None,\n",
       " 'clf__max_leaf_nodes': 751,\n",
       " 'clf__min_samples_split': 2,\n",
       " 'clf__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__n_estimators': 400,\n",
       " 'clf__oob_score': False}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space_eval(params_grid_ho, best_clf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T00:41:22.163453Z",
     "start_time": "2019-07-10T00:41:22.139431Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('preprocessing',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('pipeline-1',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value='CAT_MISSING',\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='constant',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('onehotencoder',\n",
       "                                                                   On...\n",
       "                 RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "                                        criterion='entropy', max_depth=7,\n",
       "                                        max_features=None, max_leaf_nodes=751,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=400, n_jobs=None,\n",
       "                                        oob_score=False, random_state=42,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_pipe.set_params(**space_eval(params_grid_ho, best_clf1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T22:56:48.956936Z",
     "start_time": "2019-07-09T22:56:48.951933Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['passengerid', 'survived', 'pclass', 'name', 'sex', 'age', 'sibsp',\n",
       "       'parch', 'ticket', 'fare', 'cabin', 'embarked', 'family_size', 'title',\n",
       "       'name_aspas', 'name_parenteses'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_val.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T21:42:43.335371Z",
     "start_time": "2019-07-09T21:42:39.302703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score médio nas partições de treino : 0.968869405619136\n",
      "score médio nas partições de validação : 0.872310551058348\n",
      "AUC treino: 0.964147\n",
      "AUC validação: 0.832767\n"
     ]
    }
   ],
   "source": [
    "# Cross validation no treino\n",
    "crossval = cross_validate(full_pipe, df_train, df_train['survived'], return_train_score=True, cv=kf, scoring='roc_auc')\n",
    "\n",
    "print(f\"score médio nas partições de treino : {crossval['train_score'].mean()}\")\n",
    "print(f\"score médio nas partições de validação : {crossval['test_score'].mean()}\")\n",
    "      \n",
    "full_pipe.fit(df_train, df_train['survived'])\n",
    "\n",
    "print('AUC treino: %f' % roc_auc_score(df_train['survived'], full_pipe.predict_proba(df_train)[:,1]))\n",
    "print('AUC validação: %f' % roc_auc_score(df_val['survived'], full_pipe.predict_proba(df_val)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando com Hyperopt Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T03:28:01.374540Z",
     "start_time": "2019-07-09T03:24:14.068471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 100/100 [03:47<00:00,  2.98s/it, best loss: -0.8626739511082444]\n"
     ]
    }
   ],
   "source": [
    "trials_ho_rand = Trials()\n",
    "\n",
    "best_ho_rand=fmin(partial(f_to_min1,model=full_pipe, X=df_train, y=df_train['survived'], cv=kf)\n",
    "     ,params_grid_ho, algo=rand.suggest, max_evals=100, trials=trials_ho_rand, rstate=np.random.RandomState(random_global))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T03:30:43.197894Z",
     "start_time": "2019-07-09T03:30:38.589709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score médio nas partições de treino : 0.9082833625451698\n",
      "score médio nas partições de validação : 0.8626739511082444\n",
      "AUC treino: 0.909279\n",
      "AUC validação: 0.842530\n"
     ]
    }
   ],
   "source": [
    "full_pipe.set_params(**space_eval(params_grid_ho, best_ho_rand))\n",
    "\n",
    "# Cross validation no treino\n",
    "crossval = cross_validate(full_pipe, df_train, df_train['survived'], return_train_score=True, cv=kf, scoring='roc_auc')\n",
    "\n",
    "print(f\"score médio nas partições de treino : {crossval['train_score'].mean()}\")\n",
    "print(f\"score médio nas partições de validação : {crossval['test_score'].mean()}\")\n",
    "      \n",
    "full_pipe.fit(df_train, df_train['survived'])\n",
    "\n",
    "print('AUC treino: %f' % roc_auc_score(df_train['survived'], full_pipe.predict_proba(df_train)[:,1]))\n",
    "print('AUC validação: %f' % roc_auc_score(df_val['survived'], full_pipe.predict_proba(df_val)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando com Simulated annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T03:38:29.143588Z",
     "start_time": "2019-07-09T03:34:16.307799Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 100/100 [04:12<00:00,  3.09s/it, best loss: -0.8650579444216422]\n"
     ]
    }
   ],
   "source": [
    "trials_ho_anneal = Trials()\n",
    "\n",
    "best_ho_anneal=fmin(partial(f_to_min1,model=full_pipe, X=df_train, y=df_train['survived'], cv=kf)\n",
    "     ,params_grid_ho, algo=anneal.suggest, max_evals=100, trials=trials_ho_anneal, rstate=np.random.RandomState(random_global))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T03:38:33.463521Z",
     "start_time": "2019-07-09T03:38:29.501915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score médio nas partições de treino : 0.9216896370892614\n",
      "score médio nas partições de validação : 0.8650579444216422\n",
      "AUC treino: 0.919047\n",
      "AUC validação: 0.843379\n"
     ]
    }
   ],
   "source": [
    "full_pipe.set_params(**space_eval(params_grid_ho, best_ho_anneal))\n",
    "\n",
    "# Cross validation no treino\n",
    "crossval = cross_validate(full_pipe, df_train, df_train['survived'], return_train_score=True, cv=kf, scoring='roc_auc')\n",
    "\n",
    "print(f\"score médio nas partições de treino : {crossval['train_score'].mean()}\")\n",
    "print(f\"score médio nas partições de validação : {crossval['test_score'].mean()}\")\n",
    "      \n",
    "full_pipe.fit(df_train, df_train['survived'])\n",
    "\n",
    "print('AUC treino: %f' % roc_auc_score(df_train['survived'], full_pipe.predict_proba(df_train)[:,1]))\n",
    "print('AUC validação: %f' % roc_auc_score(df_val['survived'], full_pipe.predict_proba(df_val)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando com skopt (abortado, vou usar tudo no hyperopt porque achei mais fácil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T04:02:46.179801Z",
     "start_time": "2019-07-09T04:02:46.173796Z"
    }
   },
   "outputs": [],
   "source": [
    "params_grid_ho = {\n",
    "    'clf__bootstrap': hp.choice('clf__bootstrap', [False, True]),\n",
    "    'clf__class_weight': hp.choice('clf__class_weight',['balanced', 'balanced_subsample', None]),\n",
    "    'clf__criterion': hp.choice('clf__criterion', ['gini', 'entropy']),\n",
    "    'clf__max_depth': ho_scope.int(hp.uniform('clf__max_depth', low=1, high=11)),\n",
    "    'clf__max_leaf_nodes': ho_scope.int(hp.uniform('clf__max_leaf_nodes', low=2, high=1024)),\n",
    "    'clf__min_samples_split': ho_scope.int(hp.uniform('clf__min_samples_split', low=2, high=1024)),\n",
    "    'clf__min_weight_fraction_leaf': 0.1 * ho_scope.int(hp.uniform('clf__min_weight_fraction_leaf', low=0, high=5)),\n",
    "    'clf__n_estimators': 100 * ho_scope.int(hp.uniform('clf__n_estimators', low=1, high=10)),\n",
    "    'clf__oob_score': False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T04:17:40.206509Z",
     "start_time": "2019-07-09T04:17:40.190494Z"
    }
   },
   "outputs": [],
   "source": [
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "# skopt_space  = [Integer(1, 5, name='max_depth'),\n",
    "#           Real(10**-5, 10**0, \"log-uniform\", name='learning_rate'),\n",
    "#           Integer(1, n_features, name='max_features'),\n",
    "#           Integer(2, 100, name='min_samples_split'),\n",
    "#           Integer(1, 100, name='min_samples_leaf')]\n",
    "\n",
    "skopt_space = [\n",
    "Categorical((False, True), name='clf__bootstrap'),\n",
    "Categorical(('balanced', 'balanced_subsample', None), name='clf__class_weight'),\n",
    "Integer(1, 11, name='clf__max_depth'),\n",
    "Categorical((['auto','sqrt', None]), name='clf__max_features'),\n",
    "Integer(1, 1024, name='clf__max_leaf_nodes'),\n",
    "Integer(2, 1024, name='clf__min_samples_split'),\n",
    "Real(0.1, 0.5, name='clf__min_weight_fraction_leaf'),\n",
    "Integer(100,1000, name='clf__n_estimators')\n",
    "]\n",
    "\n",
    "opt=BayesSearchCV(full_pipe, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo final pra mandar pro Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função pra submeter os resultados e salvar os arquivos necessários pra replicar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T01:01:33.799842Z",
     "start_time": "2019-07-10T01:01:33.787832Z"
    }
   },
   "outputs": [],
   "source": [
    "class SaveModel(object):\n",
    "    def __init__(self, folder_to_save, data_train=None, data_val=None, data_test=None, model=None, str_readme=None, submission_file='submission.csv'):\n",
    "        self.folder_to_save = folder_to_save\n",
    "        self.data_train = data_train\n",
    "        self.data_val = data_val\n",
    "        self.data_test = data_test\n",
    "        self.model = model\n",
    "        self.str_readme = str_readme\n",
    "        self.submission_file = submission_file\n",
    "    \n",
    "    def save_model(self):\n",
    "#     Create folder if not exists:\n",
    "        try:\n",
    "            os.makedirs(self.folder_to_save)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    #     Salva os dados usados no treino\n",
    "        if self.data_train is not None:\n",
    "            joblib.dump(self.data_train, self.folder_to_save+'/train_data')\n",
    "\n",
    "\n",
    "    #     Salva dados usados na validação\n",
    "        if self.data_val is not None:\n",
    "            joblib.dump(self.data_test, self.folder_to_save+'/validation_data')\n",
    "\n",
    "    #     Salva dados usados no teste\n",
    "        if self.data_test is not None:\n",
    "            joblib.dump(self.data_test, self.folder_to_save+'/test_data')\n",
    "\n",
    "    #     Salva modelo \n",
    "        if self.model is not None:\n",
    "            joblib.dump(self.model, self.folder_to_save+'/model')   \n",
    "\n",
    "    #     Arquivo README (é o que vai escrito pro commit)\n",
    "        with open(self.folder_to_save+'/README.txt', \"w\") as text_file:\n",
    "            text_file.write(self.str_readme)\n",
    "            \n",
    "#         Salva os predictions\n",
    "        \n",
    "\n",
    "    def commit_kaggle(self):\n",
    "        predictions = self.model.predict(self.data_test)\n",
    "        submission = pd.DataFrame({'PassengerId':self.data_test['passengerid'],'Survived':predictions})\n",
    "        submission.to_csv(self.folder_to_save+'/'+self.submission_file,index=False)\n",
    "#         print(f\"kaggle competitions submit -c titanic -f submission.csv -m \\\"{self.str_readme}\\\"\")\n",
    "#         !! f\"kaggle competitions submit -c titanic -f {self.folder_to_save+'/'+self.submission_file} -m \\\"{self.str_readme}\\\"\"\n",
    "        if os.system(f\"kaggle competitions submit -c titanic -f {self.folder_to_save+'/'+self.submission_file} -m \\\"{self.str_readme}\\\"\") != 0:\n",
    "            print('Erro submetendo o arquivo no kaggle!')\n",
    "            \n",
    "        print(f\"kaggle competitions submit -c titanic -f {self.folder_to_save+'/'+self.submission_file} -m \\\"{self.str_readme}\\\"\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando modelo só na base de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T00:42:47.106045Z",
     "start_time": "2019-07-10T00:42:46.419419Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('preprocessing',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('pipeline-1',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value='CAT_MISSING',\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='constant',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('onehotencoder',\n",
       "                                                                   On...\n",
       "                 RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "                                        criterion='entropy', max_depth=7,\n",
       "                                        max_features=None, max_leaf_nodes=751,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=400, n_jobs=None,\n",
       "                                        oob_score=False, random_state=42,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_pipe.set_params(**space_eval(params_grid_ho, best_clf1))\n",
    "full_pipe.fit(df_train, df_train['survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T00:52:27.016857Z",
     "start_time": "2019-07-10T00:52:26.643608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle competitions submit -c titanic -f ./versions_submissions/versao1/submission_versao1.csv -m \"Random forest, fitado no train data. One hot encoder pra binários, mediana pra missing numérico\"\n"
     ]
    }
   ],
   "source": [
    "modelo1 = SaveModel(folder_to_save='./versions_submissions/versao1',\n",
    "                  data_train=df_train,\n",
    "                  data_val=df_val,\n",
    "                  data_test=df_test,\n",
    "                  model = full_pipe,\n",
    "                str_readme='Random forest, fitado no train data. One hot encoder pra binários, mediana pra missing numérico',\n",
    "                submission_file='submission_versao1.csv'\n",
    "                   )\n",
    "\n",
    "modelo1.save_model()\n",
    "modelo1.commit_kaggle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T00:49:41.696131Z",
     "start_time": "2019-07-10T00:49:34.843670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted to Titanic: Machine Learning from Disaster\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/3.18k [00:00<?, ?B/s]\n",
      "100%|##########| 3.18k/3.18k [00:05<00:00, 623B/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c titanic -f ./versions_submissions/versao1/submission_versao1.csv -m \"Random forest, fitado no train data. One hot encoder pra binários, mediana pra missing numérico\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando na base toda (treino + validação)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T01:02:42.294817Z",
     "start_time": "2019-07-10T01:02:41.548138Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('preprocessing',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('pipeline-1',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value='CAT_MISSING',\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='constant',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('onehotencoder',\n",
       "                                                                   On...\n",
       "                 RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "                                        criterion='entropy', max_depth=7,\n",
       "                                        max_features=None, max_leaf_nodes=751,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=400, n_jobs=None,\n",
       "                                        oob_score=False, random_state=42,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_pipe.set_params(**space_eval(params_grid_ho, best_clf1))\n",
    "full_pipe.fit(df_train_val, df_train_val['survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T01:02:57.924886Z",
     "start_time": "2019-07-10T01:02:47.459991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle competitions submit -c titanic -f ./versions_submissions/versao2/submission_versao2.csv -m \"Random forest, fitado no train+val data. One hot encoder pra binários, mediana pra missing numérico\"\n"
     ]
    }
   ],
   "source": [
    "modelo2 = SaveModel(folder_to_save='./versions_submissions/versao2',\n",
    "                  data_train=df_train,\n",
    "                  data_val=df_val,\n",
    "                  data_test=df_test,\n",
    "                  model = full_pipe,\n",
    "                str_readme='Random forest, fitado no train+val data. One hot encoder pra binários, mediana pra missing numérico',\n",
    "                submission_file='submission_versao2.csv'\n",
    "                   )\n",
    "\n",
    "modelo2.save_model()\n",
    "modelo2.commit_kaggle()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sklearn_novo]",
   "language": "python",
   "name": "conda-env-sklearn_novo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
