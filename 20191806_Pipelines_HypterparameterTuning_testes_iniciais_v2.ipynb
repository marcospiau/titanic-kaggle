{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparei alguns modelos: logística, SVM e Random Forest. Fica mais complicado o fluxo. Fiz um pipeline legal, mas ainda falta resolver como deixar tudo otimizando usando arrays numpy e depois usar pandas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definições iniciais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T00:28:40.648596Z",
     "start_time": "2019-08-08T00:28:18.456151Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, RandomizedSearchCV, train_test_split, StratifiedKFold,cross_val_score\n",
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyperopt\n",
    "from hyperopt import fmin, hp, tpe, rand, Trials, space_eval, STATUS_OK, anneal\n",
    "from hyperopt.pyll import scope as ho_scope\n",
    "from hyperopt.pyll.stochastic import sample as ho_sample\n",
    "from functools import partial\n",
    "\n",
    "# Category encoders\n",
    "from category_encoders.hashing import HashingEncoder\n",
    "from category_encoders.sum_coding import SumEncoder\n",
    "from category_encoders.woe import WOEEncoder\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "from category_encoders.one_hot import OneHotEncoder\n",
    "from category_encoders.binary import BinaryEncoder\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Files\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "from joblib import Memory\n",
    "import joblib\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurando random pra deixar reprodutível"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T00:28:40.668615Z",
     "start_time": "2019-08-08T00:28:40.665613Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "random_global = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções úteis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função pra submeter os resultados e salvar os arquivos necessários pra replicar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T00:28:40.918590Z",
     "start_time": "2019-08-08T00:28:40.689634Z"
    }
   },
   "outputs": [],
   "source": [
    "class SaveModel(object):\n",
    "    def __init__(self, folder_to_save, data_train=None, data_val=None, data_test=None, model=None, str_readme=None, submission_file='submission.csv'):\n",
    "        self.folder_to_save = folder_to_save\n",
    "        self.data_train = data_train\n",
    "        self.data_val = data_val\n",
    "        self.data_test = data_test\n",
    "        self.model = model\n",
    "        self.str_readme = str_readme\n",
    "        self.submission_file = submission_file\n",
    "    \n",
    "    def save_model(self):\n",
    "#     Create folder if not exists:\n",
    "        try:\n",
    "            os.makedirs(self.folder_to_save)\n",
    "            print('Criando pasta %s' % self.folder_to_save)\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "\n",
    "    #     Salva os dados usados no treino\n",
    "        if self.data_train is not None:\n",
    "            joblib.dump(self.data_train, self.folder_to_save+'/train_data')\n",
    "\n",
    "\n",
    "    #     Salva dados usados na validação\n",
    "        if self.data_val is not None:\n",
    "            joblib.dump(self.data_test, self.folder_to_save+'/validation_data')\n",
    "\n",
    "    #     Salva dados usados no teste\n",
    "        if self.data_test is not None:\n",
    "            joblib.dump(self.data_test, self.folder_to_save+'/test_data')\n",
    "\n",
    "    #     Salva modelo \n",
    "        if self.model is not None:\n",
    "            joblib.dump(self.model, self.folder_to_save+'/model')   \n",
    "\n",
    "    #     Arquivo README (é o que vai escrito pro commit)\n",
    "        with open(self.folder_to_save+'/README.txt', \"w\") as text_file:\n",
    "            text_file.write(self.str_readme)\n",
    "            \n",
    "#         Salva os predictions\n",
    "        \n",
    "\n",
    "    def commit_kaggle(self):\n",
    "        predictions = self.model.predict(self.data_test)\n",
    "        submission = pd.DataFrame({'PassengerId':self.data_test['passengerid'],'Survived':predictions})\n",
    "        submission.to_csv(self.folder_to_save+'/'+self.submission_file,index=False)\n",
    "#         print(f\"kaggle competitions submit -c titanic -f submission.csv -m \\\"{self.str_readme}\\\"\")\n",
    "#         !! f\"kaggle competitions submit -c titanic -f {self.folder_to_save+'/'+self.submission_file} -m \\\"{self.str_readme}\\\"\"\n",
    "        if os.system(f\"kaggle competitions submit -c titanic -f {self.folder_to_save+'/'+self.submission_file} -m \\\"{self.str_readme}\\\"\") != 0:\n",
    "            print('Erro submetendo o arquivo no kaggle!')\n",
    "            \n",
    "        print(f\"kaggle competitions submit -c titanic -f {self.folder_to_save+'/'+self.submission_file} -m \\\"{self.str_readme}\\\"\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T00:28:41.129600Z",
     "start_time": "2019-08-08T00:28:40.933587Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TransformToDF(BaseEstimator, TransformerMixin):\n",
    "    '''Wrapper para usar transformers do sklearn mas retorn um dataframe Pandas. Projetei para usar com transformers que não\n",
    "    mudam o número de colunas na saída.'''\n",
    "    \n",
    "    def __init__(self, sklearn_transformer, return_df=True):\n",
    "        self.sklearn_transformer = sklearn_transformer\n",
    "        self.return_df = return_df\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.col_names = X.columns.values.tolist()\n",
    "            \n",
    "        self.sklearn_transformer.fit(X, y=None)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "    # assumes X is a DataFrame\n",
    "        if self.return_df:\n",
    "            return pd.DataFrame(self.sklearn_transformer.transform(X[self.col_names]), index=X.index, columns=self.col_names)\n",
    "        else:\n",
    "            return self.sklearn_transformer.transform(X)\n",
    "        \n",
    "class DFFeatureUnion(BaseEstimator,TransformerMixin):\n",
    "    # FeatureUnion but for pandas DataFrames\n",
    "    from functools import reduce\n",
    "\n",
    "    def __init__(self, transformer_list):\n",
    "        self.transformer_list = transformer_list\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for (name, t) in self.transformer_list:\n",
    "            t.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # assumes X is a DataFrame\n",
    "        Xts = [t.transform(X) for _, t in self.transformer_list]\n",
    "        Xunion = reduce(lambda X1, X2: pd.merge(X1, X2, left_index=True, right_index=True), Xts)\n",
    "        return Xunion\n",
    "    \n",
    "class ColumnExtractor(BaseEstimator,TransformerMixin):\n",
    "\n",
    "    def __init__(self, cols, return_df=True):\n",
    "        self.cols = cols\n",
    "        self.return_df = return_df\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # stateless transformer\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # assumes X is a DataFrame\n",
    "        if self.return_df:\n",
    "            return X[self.cols]\n",
    "        else:\n",
    "            return X[self.cols].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carrega dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T00:28:41.679858Z",
     "start_time": "2019-08-08T00:28:41.147617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print shape df_train_val: (891, 12)\n",
      "Print shape df_test: (418, 11)\n"
     ]
    }
   ],
   "source": [
    "df_train_val = pd.read_csv(\"./data/train.csv\")\n",
    "df_test = pd.read_csv(\"./data/test.csv\")\n",
    "df_train_val.columns = [x.lower() for x in df_train_val.columns]\n",
    "df_test.columns = [x.lower() for x in df_test.columns]\n",
    "\n",
    "print(f'Print shape df_train_val: {df_train_val.shape}')\n",
    "print(f'Print shape df_test: {df_test.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criação de algumas features novas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tamanho de família"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T00:28:42.125390Z",
     "start_time": "2019-08-08T00:28:41.835012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family_size</th>\n",
       "      <th>size</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>537</td>\n",
       "      <td>0.303538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>161</td>\n",
       "      <td>0.552795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>102</td>\n",
       "      <td>0.578431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>0.724138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>0.136364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   family_size  size      mean\n",
       "0            1   537  0.303538\n",
       "1            2   161  0.552795\n",
       "2            3   102  0.578431\n",
       "3            4    29  0.724138\n",
       "4            5    15  0.200000\n",
       "5            6    22  0.136364\n",
       "6            7    12  0.333333\n",
       "7            8     6  0.000000\n",
       "8           11     7  0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "family_size\n",
       "1     253\n",
       "2      74\n",
       "3      57\n",
       "4      14\n",
       "5       7\n",
       "6       3\n",
       "7       4\n",
       "8       2\n",
       "11      4\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_data = [df_train_val, df_test]\n",
    "for dataset in full_data:\n",
    "    dataset['family_size'] = 1 + dataset['parch'] + dataset['sibsp']\n",
    "    if 'survived' in dataset.columns:\n",
    "        display(dataset.fillna(-999).groupby('family_size')['survived'].agg(['size', 'mean']).reset_index())\n",
    "    else:\n",
    "        display(dataset.fillna(-999).groupby('family_size').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aparentemente depois de 4 familiares, talvez seja bom juntar depois"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrai o título"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T00:28:42.372616Z",
     "start_time": "2019-08-08T00:28:42.347592Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>size</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sir</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Countess</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ms</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Mme</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lady</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mlle</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Mrs</td>\n",
       "      <td>125</td>\n",
       "      <td>0.792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Miss</td>\n",
       "      <td>182</td>\n",
       "      <td>0.697802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Master</td>\n",
       "      <td>40</td>\n",
       "      <td>0.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Col</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Major</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dr</td>\n",
       "      <td>7</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mr</td>\n",
       "      <td>517</td>\n",
       "      <td>0.156673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jonkheer</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Don</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Rev</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Capt</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       title  size      mean\n",
       "16       Sir     1  1.000000\n",
       "2   Countess     1  1.000000\n",
       "14        Ms     1  1.000000\n",
       "11       Mme     1  1.000000\n",
       "6       Lady     1  1.000000\n",
       "10      Mlle     2  1.000000\n",
       "13       Mrs   125  0.792000\n",
       "9       Miss   182  0.697802\n",
       "8     Master    40  0.575000\n",
       "1        Col     2  0.500000\n",
       "7      Major     2  0.500000\n",
       "4         Dr     7  0.428571\n",
       "12        Mr   517  0.156673\n",
       "5   Jonkheer     1  0.000000\n",
       "3        Don     1  0.000000\n",
       "15       Rev     6  0.000000\n",
       "0       Capt     1  0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "title\n",
       "Col         2\n",
       "Dona        1\n",
       "Dr          1\n",
       "Master     21\n",
       "Miss       78\n",
       "Mr        240\n",
       "Mrs        72\n",
       "Ms          1\n",
       "Rev         2\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_data = [df_train_val, df_test]\n",
    "for dataset in full_data:\n",
    "    dataset['title'] = dataset['name'].str.findall('([A-Z][a-z]+)\\.').map(lambda x: x[0])\n",
    "    if 'survived' in dataset.columns:\n",
    "        display(dataset.fillna(-999).groupby('title')['survived'].agg(['size', 'mean']).reset_index().sort_values(by='mean', ascending=False))\n",
    "    else:\n",
    "        display(dataset.fillna(-999).groupby('title').size())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T02:19:52.456496Z",
     "start_time": "2019-07-09T02:19:52.417461Z"
    }
   },
   "source": [
    "## Acha aspas ou parênteses no nome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T00:28:42.649869Z",
     "start_time": "2019-08-08T00:28:42.610833Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_aspas</th>\n",
       "      <th>size</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>0.716981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>838</td>\n",
       "      <td>0.362768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name_aspas  size      mean\n",
       "1           1    53  0.716981\n",
       "0           0   838  0.362768"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_parenteses</th>\n",
       "      <th>size</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>143</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>748</td>\n",
       "      <td>0.310160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name_parenteses  size      mean\n",
       "1                1   143  0.769231\n",
       "0                0   748  0.310160"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "name_aspas\n",
       "0    396\n",
       "1     22\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "name_parenteses\n",
       "0    340\n",
       "1     78\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_data = [df_train_val, df_test]\n",
    "for dataset in full_data:\n",
    "    dataset['name_aspas'] = dataset['name'].str.findall('\\\"').map(lambda x: len(x) >= 1 ).astype('int')\n",
    "    dataset['name_parenteses'] = dataset['name'].str.findall('\\(').map(lambda x: len(x) >= 1 ).astype('int')\n",
    "    if 'survived' in dataset.columns:\n",
    "        display(dataset.fillna(-999).groupby('name_aspas')['survived'].agg(['size', 'mean']).reset_index().sort_values(by='mean', ascending=False))\n",
    "        display(dataset.fillna(-999).groupby('name_parenteses')['survived'].agg(['size', 'mean']).reset_index().sort_values(by='mean', ascending=False))\n",
    "    else:\n",
    "        display(dataset.fillna(-999).groupby('name_aspas').size())\n",
    "        display(dataset.fillna(-999).groupby('name_parenteses').size())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separa as amostras para treino e validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T00:28:42.913110Z",
     "start_time": "2019-08-08T00:28:42.905101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print shape df_train: (791, 16)\n",
      "Print shape df_val: (100, 16)\n"
     ]
    }
   ],
   "source": [
    "df_train, df_val = train_test_split(df_train_val, test_size = 100, shuffle=True, stratify=df_train_val['survived'], random_state=random_global)\n",
    "print(f'Print shape df_train: {df_train.shape}')\n",
    "print(f'Print shape df_val: {df_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T03:38:06.308453Z",
     "start_time": "2019-06-28T03:38:06.304449Z"
    }
   },
   "source": [
    "# Definição do pipeline básico a ser otimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T00:28:43.272437Z",
     "start_time": "2019-08-08T00:28:43.269432Z"
    }
   },
   "outputs": [],
   "source": [
    "num_cols = ['age', 'sibsp', 'parch', 'fare', 'family_size']\n",
    "cat_cols = ['sex', 'embarked', 'title', 'name_aspas', 'name_parenteses', 'pclass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T00:28:43.629761Z",
     "start_time": "2019-08-08T00:28:43.625757Z"
    }
   },
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T00:28:43.996649Z",
     "start_time": "2019-08-08T00:28:43.990643Z"
    }
   },
   "outputs": [],
   "source": [
    "def f_wrap_space_eval(hp_space, trial):\n",
    "    \"\"\"\n",
    "    Utility function for more consise optimization history extraction\n",
    "    \n",
    "    Parameters:\n",
    "    ----------------\n",
    "    hp_space : hyperspace from which points are sampled\n",
    "    trial : hyperopt.Trials object\n",
    "    \n",
    "    Returns:\n",
    "    ----------------\n",
    "    : dict(\n",
    "        k: v\n",
    "    ), where k - label of hyperparameter, v - value of hyperparameter in trial\n",
    "    \"\"\"\n",
    "    \n",
    "    return space_eval(hp_space, {k: v[0] for (k, v) in trial['misc']['vals'].items() if len(v) > 0})\n",
    "\n",
    "\n",
    "def f_unpack_dict(dct):\n",
    "    \"\"\"\n",
    "    Unpacks all sub-dictionaries in given dictionary recursively. There should be no duplicated keys \n",
    "    across all nested subdictionaries, or some instances will be lost without warning\n",
    "    \n",
    "    Parameters:\n",
    "    ----------------\n",
    "    dct : dictionary to unpack\n",
    "    \n",
    "    Returns:\n",
    "    ----------------\n",
    "    : unpacked dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    res = {}\n",
    "    for (k, v) in dct.items():\n",
    "        if isinstance(v, dict):\n",
    "            res = {**res, **f_unpack_dict(v)}\n",
    "        else:\n",
    "            res[k] = v\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T20:24:31.531103Z",
     "start_time": "2019-07-13T20:13:57.407580Z"
    }
   },
   "outputs": [],
   "source": [
    "trials_clf2 = Trials()\n",
    "best_clf2 = fmin(partial(f_to_min2, X=df_train[num_cols].fillna(0), y=df_train['survived']), \n",
    "                 hp_space_clf2, algo=tpe.suggest, max_evals=500, \n",
    "                 trials=trials_clf2, rstate=np.random.RandomState(random_global))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T03:59:26.286088Z",
     "start_time": "2019-07-14T03:59:24.694643Z"
    }
   },
   "outputs": [],
   "source": [
    "fig0 = plt.figure(figsize=(16, 8))\n",
    "gs = gridspec.GridSpec(nrows=4, ncols=4)\n",
    "\n",
    "# ================================\n",
    "# Plotting optimization history\n",
    "\n",
    "ax = fig0.add_subplot(gs[:2, :])\n",
    "ax.plot(range(1, len(trials_clf2) + 1), [-x['result']['loss'] for x in trials_clf2], \n",
    "        color='red', marker='.', linewidth=0)\n",
    "\n",
    "ax.set_xlabel('Iteration', fontsize=12)\n",
    "ax.set_ylabel('ROC-AUC', fontsize=12)\n",
    "ax.set_title('Optimization history', fontsize=14)\n",
    "\n",
    "ax.grid(True)\n",
    "\n",
    "# ================================\n",
    "# Plotting sampled points\n",
    "\n",
    "samples = [f_unpack_dict(f_wrap_space_eval(hp_space_clf2, x)) for x in trials_clf2.trials]\n",
    "\n",
    "ax = fig0.add_subplot(gs[2:, 0])\n",
    "sns.countplot(x=[x['type'] for x in samples], ax=ax)\n",
    "\n",
    "ax.set_xlabel('Classifier type', fontsize=12)\n",
    "ax.set_ylabel('Counts', fontsize=12)\n",
    "\n",
    "ax.grid(True)\n",
    "\n",
    "# ----------------\n",
    "\n",
    "ax = fig0.add_subplot(gs[2:, 1:3])\n",
    "ax.hist(np.log([x['C'] for x in samples if x['type'] == 'SVM']), bins=20, rwidth=0.9, \n",
    "        range=(-4*np.log(10), 4*np.log(10)))\n",
    "\n",
    "ax.set_xlabel('SVM, $\\ln C$', fontsize=12)\n",
    "ax.set_ylabel('Counts', fontsize=12)\n",
    "\n",
    "ax.grid(True)\n",
    "\n",
    "# ----------------\n",
    "\n",
    "ax = fig0.add_subplot(gs[2:, 3])\n",
    "ax.hist(np.log([x['C'] for x in samples if x['type'] == 'logit']), bins=20, rwidth=0.9, \n",
    "        range=(-4*np.log(10), 4*np.log(10)))\n",
    "\n",
    "ax.set_xlabel('logit, $\\ln C$', fontsize=12)\n",
    "ax.set_ylabel('Counts', fontsize=12)\n",
    "\n",
    "ax.grid(True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T04:58:57.624308Z",
     "start_time": "2019-07-14T04:58:57.601296Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pipeline com Pandas dataframes em todas as etapas, o que deixar mais devagar do que usar só numpy ndarrays.\n",
    "# Minha ideia inicial era deixar a otimização usando numpy ndarrays e na rodada final usar pandas pra conseguir\n",
    "# ver feature importances e ver os dados finais que saem do pipeline, após as transformações.\n",
    "# Empecilhos para isso acontecer:\n",
    "#     -os encoders do category_encoders foram feitos para serem usados com pandas dataframes, e prcisam dos nomes\n",
    "#     das colunas, caso contrário pegam todas colunas do tipo 'object';\n",
    "#     -na hora de juntar as features dos fluxos de variáveis categóricas e contínuas, ou uso o FeatureUnion (merge\n",
    "#     em numpy arrays) ou o DFFeatureUnion (merge em dataframes Pandas)\n",
    "\n",
    "hp_space_clf3 = {\n",
    "    'num_missing_strategy': hp.choice('num_missing_strategy', ['mean', 'median', 'constant']),\n",
    "    'categ_encoder': hp.choice('categ_encoder', [WOEEncoder(cols=cat_cols, return_df=True)\n",
    "                                                 ,OneHotEncoder(cols=cat_cols, use_cat_names=True, return_df=True)\n",
    "                                                 ,TargetEncoder(cols=cat_cols, return_df=True)])\n",
    "    ,\n",
    "    # type refers to classifier type: either logit or SVM\n",
    "    'clf_type': hp.choice('clf_type', [\n",
    "        {\n",
    "            'type': 'logit',\n",
    "            'clf': {\n",
    "                'C': hp.loguniform('logit.C', -4.0*np.log(10.0), 4.0*np.log(10.0)), \n",
    "                'class_weight': hp.choice('logit.class_weight', [None, 'balanced'])\n",
    "            }\n",
    "        }, \n",
    "        {\n",
    "            'type': 'SVM', \n",
    "            'clf': {\n",
    "                'C': hp.loguniform('svm.C', -4.0*np.log(10.0), 4.0*np.log(10.0)), \n",
    "                'class_weight': hp.choice('svm.class_weight', [None, 'balanced']), \n",
    "                'kernel': 'rbf', \n",
    "                'gamma': hp.choice('svm.gamma', ['auto', 'scale'])\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'type': 'RandomForest', \n",
    "            'clf': {\n",
    "                    'bootstrap': hp.choice('rf.bootstrap', [False, True]),\n",
    "                    'class_weight': hp.choice('rf.class_weight',['balanced', 'balanced_subsample', None]),\n",
    "                    'criterion': hp.choice('rf.criterion', ['gini', 'entropy']),\n",
    "                    'max_depth': ho_scope.int(hp.uniform('rf.max_depth', low=1, high=11)),\n",
    "                    'max_features': hp.choice('rf.max_features', ['auto','sqrt', None]),\n",
    "                    'max_leaf_nodes': ho_scope.int(hp.uniform('rf.max_leaf_nodes', low=2, high=1024)),\n",
    "                    'min_samples_split': ho_scope.int(hp.uniform('rf.min_samples_split', low=2, high=1024)),\n",
    "                    'min_weight_fraction_leaf': 0.1 * ho_scope.int(hp.uniform('rf.min_weight_fraction_leaf', low=0, high=5)),\n",
    "                    'n_estimators': 100 * ho_scope.int(hp.uniform('rf.n_estimators', low=1, high=10)),\n",
    "                    'oob_score': False\n",
    "            }\n",
    "        }\n",
    "    ])\n",
    "}\n",
    "\n",
    "def f_clf3(hps):\n",
    "    \"\"\"\n",
    "    Constructs estimator\n",
    "    \n",
    "    Parameters:\n",
    "    ----------------\n",
    "    hps : sample point from search space\n",
    "    \n",
    "    Returns:\n",
    "    ----------------\n",
    "    model : sklearn.Pipeline.pipeline with hyperparameters set up as per hps\n",
    "    \"\"\"\n",
    "    cat_pipeline = make_pipeline(ColumnExtractor(cat_cols, return_df=True), TransformToDF(SimpleImputer(strategy='constant', fill_value='CAT_MISSING')\n",
    "                                                                          , return_df=True), hps['categ_encoder'])\n",
    "    num_pipeline = make_pipeline(ColumnExtractor(num_cols, return_df=True), TransformToDF(SimpleImputer(fill_value=0, strategy=hps['num_missing_strategy'])\n",
    "                                                                          , return_df=True),TransformToDF(StandardScaler()))\n",
    "    \n",
    "    if hps['clf_type']['type'] == 'logit':\n",
    "        clf = LogisticRegression(**hps['clf_type']['clf'], solver='sag', max_iter=25000, random_state=random_global)\n",
    "    elif hps['clf_type']['type'] == 'SVM':\n",
    "        clf = SVC(**f_unpack_dict(hps['clf_type']['clf']), probability=True, random_state=random_global)\n",
    "    elif hps['clf_type']['type'] == 'RandomForest':\n",
    "        clf = RandomForestClassifier(**f_unpack_dict(hps['clf_type']['clf']), random_state=random_global)\n",
    "    else:\n",
    "        raise KeyError('Unknown classifier type hyperparameter value: {0}'.format(hps['clf_type']['type']))\n",
    "    \n",
    "    model = Pipeline([('features', DFFeatureUnion([('cat_flow',cat_pipeline), ('num_flow',num_pipeline)])), ('clf', clf)])\n",
    "    return model\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def f_to_min3(hps, X, y, ncv=kf):\n",
    "    \"\"\"\n",
    "    Target function for optimization\n",
    "    \n",
    "    Parameters:\n",
    "    ----------------\n",
    "    hps : sample point from search space\n",
    "    X : feature matrix\n",
    "    y : target array\n",
    "    ncv : number of folds for cross-validation\n",
    "    \n",
    "    Returns:\n",
    "    ----------------\n",
    "    : dict(\n",
    "        'loss' : target function value (negative mean cross-validation ROC-AUC score)\n",
    "        'cv_std' : cross-validation ROC-AUC score standard deviation\n",
    "        'status' : status of function evaluation\n",
    "    )\n",
    "    \"\"\"\n",
    "    model = f_clf3(hps)\n",
    "    cv_res = cross_val_score(model, X, y, cv=ncv, scoring='roc_auc')\n",
    "    \n",
    "    return {\n",
    "        'loss': -cv_res.mean(), \n",
    "        'cv_std': cv_res.std(), \n",
    "        'status': STATUS_OK\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T05:16:24.079560Z",
     "start_time": "2019-07-14T04:59:11.477505Z"
    }
   },
   "outputs": [],
   "source": [
    "# Retornando DF em tudo - sem usar Memory\n",
    "trials_clf3 = Trials()\n",
    "best_clf3 = fmin(partial(f_to_min3, X=df_train[num_cols+cat_cols], y=df_train['survived']), \n",
    "                 hp_space_clf3, algo=tpe.suggest, max_evals=500, \n",
    "                 trials=trials_clf3, rstate=np.random.RandomState(random_global))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T04:39:14.192645Z",
     "start_time": "2019-07-14T04:39:14.189642Z"
    }
   },
   "outputs": [],
   "source": [
    "cachedir = mkdtemp(dir='./')\n",
    "memory = Memory(location=cachedir, verbose=0)\n",
    "# rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T04:17:17.625502Z",
     "start_time": "2019-07-14T04:17:17.622509Z"
    }
   },
   "outputs": [],
   "source": [
    "rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T04:26:42.549876Z",
     "start_time": "2019-07-14T04:26:42.541867Z"
    }
   },
   "outputs": [],
   "source": [
    "space_eval(hp_space_clf3,best_clf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T04:01:28.839639Z",
     "start_time": "2019-07-14T04:01:22.806863Z"
    }
   },
   "outputs": [],
   "source": [
    "full_pipe = f_clf3(space_eval(hp_space_clf3,best_clf3))\n",
    "full_pipe\n",
    "\n",
    "# Cross validation no treino\n",
    "crossval = cross_validate(full_pipe, df_train, df_train['survived'], return_train_score=True, cv=kf, scoring='roc_auc')\n",
    "\n",
    "print(f\"score médio nas partições de treino : {crossval['train_score'].mean()}\")\n",
    "print(f\"score médio nas partições de validação : {crossval['test_score'].mean()}\")\n",
    "      \n",
    "full_pipe.fit(df_train, df_train['survived'])\n",
    "\n",
    "print('AUC treino: %f' % roc_auc_score(df_train['survived'], full_pipe.predict_proba(df_train)[:,1]))\n",
    "print('AUC validação: %f' % roc_auc_score(df_val['survived'], full_pipe.predict_proba(df_val)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T03:32:55.993122Z",
     "start_time": "2019-07-14T03:32:55.990120Z"
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt.plotting import main_plot_1D_attachment,main_plot_histogram,main_plot_history,main_plot_vars "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T03:32:56.999108Z",
     "start_time": "2019-07-14T03:32:56.791911Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "main_plot_1D_attachment(trials=trials_clf3, attachment_name='clf_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T04:02:19.965465Z",
     "start_time": "2019-07-14T04:02:19.798314Z"
    }
   },
   "outputs": [],
   "source": [
    "main_plot_histogram(trials=trials_clf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T04:02:24.125048Z",
     "start_time": "2019-07-14T04:02:23.862454Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "main_plot_history(trials=trials_clf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T04:02:28.043543Z",
     "start_time": "2019-07-14T04:02:26.518092Z"
    }
   },
   "outputs": [],
   "source": [
    "# Here is a small example:\n",
    "from hyperopt.plotting import main_plot_vars\n",
    "from hyperopt import base\n",
    "\n",
    "# setting up your search_space, objective_function, and trials goes here\n",
    "# see the tutorials to know how\n",
    "plt.figure(figsize=(20,10))\n",
    "main_plot_vars(trials=trials_clf3, colorize_best=50, columns=3, arrange_by_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T04:02:32.260470Z",
     "start_time": "2019-07-14T04:02:30.510880Z"
    }
   },
   "outputs": [],
   "source": [
    "fig0 = plt.figure(figsize=(16, 8))\n",
    "gs = gridspec.GridSpec(nrows=4, ncols=4)\n",
    "\n",
    "# ================================\n",
    "# Plotting optimization history\n",
    "\n",
    "ax = fig0.add_subplot(gs[:2, :])\n",
    "ax.plot(range(1, len(trials_clf3) + 1), [-x['result']['loss'] for x in trials_clf3], \n",
    "        color='red', marker='.', linewidth=0)\n",
    "\n",
    "ax.set_xlabel('Iteration', fontsize=12)\n",
    "ax.set_ylabel('ROC-AUC', fontsize=12)\n",
    "ax.set_title('Optimization history', fontsize=14)\n",
    "\n",
    "ax.grid(True)\n",
    "\n",
    "# ================================\n",
    "# Plotting sampled points\n",
    "\n",
    "samples = [f_unpack_dict(f_wrap_space_eval(hp_space_clf3, x)) for x in trials_clf3.trials]\n",
    "\n",
    "ax = fig0.add_subplot(gs[2:, 0])\n",
    "sns.countplot(x=[x['type'] for x in samples], ax=ax)\n",
    "\n",
    "ax.set_xlabel('Classifier type', fontsize=12)\n",
    "ax.set_ylabel('Counts', fontsize=12)\n",
    "\n",
    "ax.grid(True)\n",
    "\n",
    "# ----------------\n",
    "\n",
    "ax = fig0.add_subplot(gs[2:, 1:2])\n",
    "ax.hist(np.log([x['C'] for x in samples if x['type'] == 'SVM']), bins=20, rwidth=0.9, \n",
    "        range=(-4*np.log(10), 4*np.log(10)))\n",
    "\n",
    "ax.set_xlabel('SVM, $\\ln C$', fontsize=12)\n",
    "ax.set_ylabel('Counts', fontsize=12)\n",
    "\n",
    "ax.grid(True)\n",
    "\n",
    "# ----------------\n",
    "\n",
    "ax = fig0.add_subplot(gs[2:, 3])\n",
    "ax.hist(np.log([x['C'] for x in samples if x['type'] == 'logit']), bins=20, rwidth=0.9, \n",
    "        range=(-4*np.log(10), 4*np.log(10)))\n",
    "\n",
    "ax.set_xlabel('logit, $\\ln C$', fontsize=12)\n",
    "ax.set_ylabel('Counts', fontsize=12)\n",
    "\n",
    "ax.grid(True)\n",
    "\n",
    "# ----------------\n",
    "\n",
    "ax = fig0.add_subplot(gs[2:, 2:3])\n",
    "ax.hist(np.log([x['max_leaf_nodes'] for x in samples if x['type'] == 'RandomForest']), bins=20, rwidth=0.9, \n",
    "        range=(-4*np.log(10), 4*np.log(10)))\n",
    "ax.set_xlabel('max_leaf_nodes', fontsize=12)\n",
    "ax.set_ylabel('Counts', fontsize=12)\n",
    "\n",
    "ax.grid(True)\n",
    "\n",
    "# # ================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo final pra mandar pro Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T03:35:05.857982Z",
     "start_time": "2019-07-14T03:35:05.854980Z"
    }
   },
   "source": [
    "## Treinando modelo só na base de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T04:04:13.297885Z",
     "start_time": "2019-07-14T04:04:13.286875Z"
    }
   },
   "outputs": [],
   "source": [
    "full_pipe.named_steps['features'].transformer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T04:02:46.921534Z",
     "start_time": "2019-07-14T04:02:45.933904Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "full_pipe = f_clf3(space_eval(hp_space_clf3,best_clf3))\n",
    "full_pipe.fit(df_train[num_cols+cat_cols], df_train['survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T04:10:26.492077Z",
     "start_time": "2019-07-14T04:10:14.694069Z"
    }
   },
   "outputs": [],
   "source": [
    "modelo3 = SaveModel(folder_to_save='./versions_submissions/versao3',\n",
    "                  data_train=df_train,\n",
    "                  data_val=df_val,\n",
    "                  data_test=df_test,\n",
    "                  model = full_pipe,\n",
    "                str_readme='Random forest, fitado no train data. WOE encoder pra binários, mediana pra missing numérico',\n",
    "                submission_file='submission_versao3.csv'\n",
    "                   )\n",
    "\n",
    "modelo3.save_model()\n",
    "modelo3.commit_kaggle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando na base toda (treino + validação)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T04:11:36.705727Z",
     "start_time": "2019-07-14T04:11:35.629589Z"
    }
   },
   "outputs": [],
   "source": [
    "full_pipe = f_clf3(space_eval(hp_space_clf3,best_clf3))\n",
    "full_pipe.fit(df_train_val[num_cols+cat_cols], df_train_val['survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T04:12:34.228355Z",
     "start_time": "2019-07-14T04:12:33.510693Z"
    }
   },
   "outputs": [],
   "source": [
    "modelo4 = SaveModel(folder_to_save='./versions_submissions/versao4',\n",
    "                  data_train=df_train,\n",
    "                  data_val=df_val,\n",
    "                  data_test=df_test,\n",
    "                  model = full_pipe,\n",
    "                str_readme='Random forest, fitado no train + val data. WOE encoder pra binários, mediana pra missing numérico',\n",
    "                submission_file='submission_versao3.csv'\n",
    "                   )\n",
    "\n",
    "modelo4.save_model()\n",
    "modelo4.commit_kaggle()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sklearn_novo]",
   "language": "python",
   "name": "conda-env-sklearn_novo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
